{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iGeology-Illinois/geol-581-module-3-principles-of-uncertainty-chapter-4-GouldDirt26/blob/main/GEOL_593_Alex_M_Gould_Module_3_HW_FEB232026.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjg3u46e670c"
      },
      "source": [
        "# Question 4.1\n",
        "\n",
        "A rock slope is to be cut as part of a road construction project. However, there is a risk that the slope could slide along the bedding planes into the excavation pit. In order to carry out a stability analysis it is necessary to estimate the angle of friction along the bedding planes. An initial assessment on the mean value of the friction angle can be obtained from the spectrum of possible manifestations of the friction angle. It cannot be smaller than the base friction angle, i.e. the friction angle that would result on the sawn, smooth surface of a specimen. The base friction angle for the greywackes that make up the slope is about 20°. The maximum possible friction angle can be estimated by back-calculating past failure cases. So far, no friction angle exceeding 35° has been back-calculated. What is the probability that the friction angle is smaller than 25°? Since no further information is available, the PDF of the friction angle can be assumed to be uniform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0na3blj670d",
        "outputId": "771244ef-a42f-481e-cb98-1f9178dd7481"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utilizing a uniform distibution (Scripy). The proability the friction for the rock slope is less than 25° is 0.3333 or 33.3%\n",
            " \n",
            "Utilizing an analytical approach [e.g., (25 - 20) / (35 - 20)]. The proability the friction for the rock slope is less than 25° is 0.3333 or 33.3%\n",
            " \n",
            "Regardless of the method choosen, the probability the angle of friction lower than an analyzed angle is the same .\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# STEP 1 - Define the parameters for the uniform distribution by bounds\n",
        "Angle_lower = 20\n",
        "Angle_upper = 35\n",
        "\n",
        "# STEP 2 - Define the desired angle\n",
        "Angle_desired = 25\n",
        "\n",
        "# STEP 3 - Create distributions\n",
        "  # Uniform\n",
        "uniform_dist = stats.uniform(loc=Angle_lower, scale=Angle_upper - Angle_lower)\n",
        "  # Analytical\n",
        "probability_scipy = uniform_dist.cdf(Angle_desired)\n",
        "\n",
        "# STEP 4 - Calculate the probability using the analytical approach\n",
        "probability_analytical = (Angle_desired - Angle_lower) / (Angle_upper - Angle_lower)\n",
        "\n",
        "# STEP 5 - Provide call back for analysis\n",
        "print(f\"Utilizing a uniform distibution (Scripy). The proability the friction for the rock slope is less than {Angle_desired}° is {probability_scipy:.4f} or {probability_scipy:.1%}\")\n",
        "print(f\" \")\n",
        "print(f\"Utilizing an analytical approach [e.g., ({Angle_desired} - {Angle_lower}) / ({Angle_upper} - {Angle_lower})]. The proability the friction for the rock slope is less than {Angle_desired}° is {probability_analytical:.4f} or {probability_analytical:.1%}\")\n",
        "print(f\" \")\n",
        "# STEP6 - Verify Results\n",
        "assert np.isclose(probability_scipy, probability_analytical), \"Results Differ!\"\n",
        "print(\"Regardless of the method choosen, the probability the angle of friction lower than an analyzed angle is the same .\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75NWhcc7670d"
      },
      "source": [
        "# Question 4.2\n",
        "\n",
        "A mobile point load tester can be used to determine a strength index \\(I_p\\) [MPa] for rock samples, which are correlated with the uniaxial compressive strength. 25 strength values \\(I_p\\) were determined for a sandstone sequence:\n",
        "\n",
        "4.4; 4.2; 4.5; 4.0; 4.3; 4.1; 3.9; 4.2; 4.5; 3.8; 4.0; 4.3; 4.2; 4.2;\n",
        "4.1; 4.5; 4.2; 4.3; 4.1; 3.7; 4.0; 4.5; 4.6; 4.3; 4.1\n",
        "(n = 25)\n",
        "\n",
        "In the neighboring mapping area, another 10 strength coefficients for sandstone were determined:\n",
        "\n",
        "3.7; 4.5; 4.2; 4.4; 4.3; 4.0; 3.5; 3.9; 4.3; 4.3\n",
        "(n = 10)\n",
        "\n",
        "Do both samples come from the same population?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "FfWPWA_b670e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bf750de-9d97-4256-fdd1-63c099de205c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the collected strength coefficients of AREA 1, NOTE:separate values by a space or semicolon (i.e., 1.2; 2.3; 3.4; 4.5): 4.4; 4.2; 4.5; 4.0; 4.3; 4.1; 3.9; 4.2; 4.5; 3.8; 4.0; 4.3; 4.2; 4.2; 4.1; 4.5; 4.2; 4.3; 4.1; 3.7; 4.0; 4.5; 4.6; 4.3; 4.1\n",
            "Enter the collected strength coefficients of AREA 2, NOTE:separate values by a space or semicolon (i.e., 1.2; 2.3; 3.4; 4.5): 3.7; 4.5; 4.2; 4.4; 4.3; 4.0; 3.5; 3.9; 4.3; 4.3\n",
            " \n",
            "Data set 1 has strength coefficients of: 4.4  4.2  4.5  4.0  4.3  4.1  3.9  4.2  4.5  3.8  4.0  4.3  4.2  4.2  4.1  4.5  4.2  4.3  4.1  3.7  4.0  4.5  4.6  4.3  4.1\n",
            "Data set 2 has strength coefficients of: 3.7  4.5  4.2  4.4  4.3  4.0  3.5  3.9  4.3  4.3\n",
            "\n",
            "--- T-Test of Strength Coefficients ---\n",
            "T-statistic: 0.8004\n",
            "P-value: 0.4382\n",
            "What significance level would you like to use?: 0.05\n",
            "Since the p-value (0.4382) is greater than the significance level (0.05), and therefore we are unable to reject our hypothesis.\n",
            "Based on our statistical analysis it is likely the two provided data sets are from the same populations.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# STEP 1 - Provide strength coefficients for location 1 and location 2\n",
        "Strength_1 = input(\"Enter the collected strength coefficients of AREA 1, NOTE:separate values by a space or semicolon (i.e., 1.2; 2.3; 3.4; 4.5): \")\n",
        "Strength_2 = input(\"Enter the collected strength coefficients of AREA 2, NOTE:separate values by a space or semicolon (i.e., 1.2; 2.3; 3.4; 4.5): \")\n",
        "\n",
        "# Replace semicolons with spaces to standardize the separator\n",
        "Strength_1_processed = Strength_1.replace(';', ' ')\n",
        "Strength_2_processed = Strength_2.replace(';', ' ')\n",
        "print(\" \")\n",
        "print(\"Data set 1 has strength coefficients of:\", Strength_1_processed)\n",
        "print(\"Data set 2 has strength coefficients of:\", Strength_2_processed)\n",
        "\n",
        "# Convert processed strength coefficients to NumPy arrays\n",
        "Strength_1_array = np.array([float(x) for x in Strength_1_processed.split() if x.strip()])\n",
        "Strength_2_array = np.array([float(x) for x in Strength_2_processed.split() if x.strip()])\n",
        "\n",
        "# STEP 3 - Perform a two-sample independent t-test\n",
        "t_statistic, p_value = stats.ttest_ind(Strength_1_array, Strength_2_array, equal_var=False)\n",
        "\n",
        "print(f\"\\n--- T-Test of Strength Coefficients ---\")\n",
        "print(f\"T-statistic: {t_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# STEP 4 - Select signifigance level\n",
        "Significance_level_str = input(\"What significance level would you like to use?: \")\n",
        "\n",
        "try:\n",
        "    Significance_level = float(Significance_level_str)\n",
        "    if p_value < Significance_level:\n",
        "        print(f\"Since the p-value ({p_value:.4f}) is less than the significance level ({Significance_level}), and therefore our hypothesis is null.\")\n",
        "        print(f\"Based on our statistical analysis it is likely the two provided data sets are from different populations.\")\n",
        "    else:\n",
        "        print(f\"Since the p-value ({p_value:.4f}) is greater than the significance level ({Significance_level}), and therefore we are unable to reject our hypothesis.\")\n",
        "        print(f\"Based on our statistical analysis it is likely the two provided data sets are from the same populations.\")\n",
        "except ValueError:\n",
        "    print(f\"Invalid input for significance level: '{Significance_level_str}'. Please enter a valid number.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vOVmOaS670e"
      },
      "outputs": [],
      "source": [
        "# Question 4.3\n",
        "\n",
        "A construction pit of 10,000 m³ is to be excavated. The subground consists of Pleistocene sediments of which it is known that erratic blocks (boulders) occur, having an average diameter of 1.5 m.\n",
        "Experience shows that about 1% by volume of the excavated material in this region consists of boulders. Since special equipment is needed to extract them and delays in construction are to be expected, the contractor\n",
        "is interested in the probability of having to extract more boulders than experience suggests. He would also like to know what the probability is that more than ten boulders will be found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "kIcwou-T670e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ca21e6-8e36-466f-a443-4bb1a8c61a71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the total volume of excavation in cubic meters (m^3): 10000\n",
            "Enter the percentage of boulders in the excavation volume (in decimal form): 0.01\n"
          ]
        }
      ],
      "source": [
        "# In this cell, you can set up a binomial or Poisson approach (depending on your modeling assumptions).\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# STEP 1 - Estimate the expected number of boulders as 1% of the total volume or total 'units' of excavation.\n",
        "Excavation_vol = input(\"Enter the total volume of excavation in cubic meters (m^3): \")\n",
        "Boulder_percent = input(\"Enter the percentage of boulders in the excavation volume (in decimal form): \")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16dc58d9",
        "outputId": "51a0a075-429d-49d5-bd0b-3c42be8d7532"
      },
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Convert inputs from strings to floats\n",
        "excavation_volume = float(Excavation_vol)\n",
        "boulder_percentage = float(Boulder_percent)\n",
        "\n",
        "# --- Parameters for Binomial Distribution ---\n",
        "\n",
        "# Average boulder diameter\n",
        "boulder_diameter = 1.5 # meters\n",
        "boulder_radius = boulder_diameter / 2\n",
        "\n",
        "# Calculate the approximate volume of a single boulder (assuming spherical)\n",
        "volume_of_single_boulder = (4/3) * np.pi * (boulder_radius**3)\n",
        "print(f\"Approximate volume of a single boulder: {volume_of_single_boulder:.2f} m^3\")\n",
        "\n",
        "# 'n' (number of trials) for the binomial model can be estimated as the total excavation volume\n",
        "# divided by the volume of a single boulder. This represents the number of 'boulder-sized slots'.\n",
        "n_trials = int(round(excavation_volume / volume_of_single_boulder))\n",
        "\n",
        "# 'p' (probability of success) is the volumetric percentage of boulders\n",
        "p_success = boulder_percentage\n",
        "\n",
        "print(f\"Total excavation volume: {excavation_volume} m^3\")\n",
        "print(f\"Boulder percentage (by volume): {p_success:.2%}\")\n",
        "print(f\"Estimated number of trials (n): {n_trials}\")\n",
        "print(f\"Probability of a 'slot' containing a boulder (p): {p_success}\")\n",
        "\n",
        "# Expected number of boulders based on experience\n",
        "expected_boulders = excavation_volume * p_success / volume_of_single_boulder\n",
        "print(f\"Expected number of boulders from experience: {expected_boulders:.2f}\")\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Approximate volume of a single boulder: 1.77 m^3\n",
            "Total excavation volume: 10000.0 m^3\n",
            "Boulder percentage (by volume): 1.00%\n",
            "Estimated number of trials (n): 5659\n",
            "Probability of a 'slot' containing a boulder (p): 0.01\n",
            "Expected number of boulders from experience: 56.59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a3cef95",
        "outputId": "015caf7f-3d06-4d9a-9306-963862a1c397"
      },
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "# Binomial Model\n",
        "binomial_dist = stats.binom(n=n_trials, p=p_success)\n",
        "prob_more_than_ten_binom = 1 - binomial_dist.cdf(10)\n",
        "print(f\"Probability of more than ten boulders: {prob_more_than_ten_binom:.4f}\")\n",
        "\n",
        "# Poisson Approximation\n",
        "lambda_poisson = expected_boulders\n",
        "poisson_dist = stats.poisson(mu=lambda_poisson)\n",
        "prob_more_than_ten_poisson = 1 - poisson_dist.cdf(10)\n",
        "print(f\"Probability of more than ten boulders: {prob_more_than_ten_poisson:.4f}\")\n",
        "\n",
        "print(\"Based on our analysis it is very likely that more than 10 boulders will be encountered and the contractor should put special contingencys in place for the removal of boulders.\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of more than ten boulders: 1.0000\n",
            "Probability of more than ten boulders: 1.0000\n",
            "Based on our analysis it is very likely that more than 10 boulders will be encountered and the contractor should put special contingencys in place for the removal of boulders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40WCSa87670e"
      },
      "source": [
        "# Question 4.4\n",
        "\n",
        "In the Devonian sedimentary bedrock of the Rhenish Massif (Germany), bedding plane distances were measured for four different stratigraphic units. The coefficients of (squared) skewness and kurtosis are given as:\n",
        "\n",
        "\\[\n",
        "(\\beta_1^2, \\beta_2) = (1.82, 4.85);\\quad (1.00, 3.35);\\quad (2.72, 5.71);\\quad (0.52, 2.95).\n",
        "\\]\n",
        "\n",
        "What statistical distribution do the bedding plane distances follow?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eO4uuvGp670e"
      },
      "outputs": [],
      "source": [
        "# In this cell, consider how to analyze a dataset to infer its distribution based on skewness and kurtosis.\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# HINT:\n",
        "# 1. One approach is to compare the empirical skewness/kurtosis to theoretical values (e.g., normal, lognormal, gamma).\n",
        "# 2. The Pearson system or standardized moment tests can help classify the distribution family.\n",
        "# 3. If you had actual data, you could apply stats.skew(...) and stats.kurtosis(...) and compare.\n",
        "\n",
        "# Example steps you might take:\n",
        "# measured_skew_kurt = [(1.82, 4.85), (1.00, 3.35), (2.72, 5.71), (0.52, 2.95)]\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob6M_dco670e"
      },
      "source": [
        "### Question 4.5\n",
        "In a valley, a bone fragment of a previously unknown Cretaceous species of ichthyosaur was found during a geological excursion. To search for more fragments, you plan to investigate two upstream branches of the river. Branch 1 (larger) has a catchment area of 18 km², while Branch 2 (smaller) has a catchment area of 10 km². Additionally, in 35% of Branch 1’s area marine Cretaceous rocks are exposed, whereas in Branch 2 that figure is 80%. Given these data, estimate the probability that the fossil came from the larger catchment area. Explain any assumptions you make about probabilities and how you handle the likelihood of the fossil being transported from each branch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoAMNemV670e"
      },
      "outputs": [],
      "source": [
        "# Hints and Starting Code for Question 4.5\n",
        "\n",
        "# You might want to import basic libraries such as numpy and math for probability calculations:\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Approach Hints:\n",
        "# 1. Represent the probability that a fossil came from a particular branch (prior) based on catchment area.\n",
        "# 2. Update that probability using the conditional probability of the fossil being marine Cretaceous\n",
        "#    (which depends on the fraction of each catchment area exposing marine rocks).\n",
        "# 3. Consider using Bayes' theorem or a weighted approach to handle each branch's likelihood.\n",
        "\n",
        "# You can define variables like:\n",
        "# area_branch1 = 18\n",
        "# area_branch2 = 10\n",
        "# marine_fraction_branch1 = 0.35\n",
        "# marine_fraction_branch2 = 0.80\n",
        "# Then calculate the updated probability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhgTFvlZ670f"
      },
      "source": [
        "### Question 4.6\n",
        "Extensive shear strength studies on marine sand indicate that its friction angle follows a lognormal distribution. The measured mean value and standard deviation of the log-transformed friction angle are reported as μ<sub>lnφ</sub> = 3.25 and σ<sub>lnφ</sub> = 0.65. Use these parameters to discuss the distribution of friction angles and to compute key statistics (e.g., the mean friction angle in degrees and its confidence intervals) assuming a lognormal model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b9Wo8mR670f"
      },
      "outputs": [],
      "source": [
        "# Hints and Starting Code for Question 4.6\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.stats import lognorm\n",
        "\n",
        "# Approach Hints:\n",
        "# 1. Recall that if X is lognormally distributed, then ln(X) ~ N(mean, std).\n",
        "# 2. Here, mean = 3.25, std = 0.65 refer to the normal distribution of ln(φ).\n",
        "# 3. Use scipy.stats.lognorm functions or manual transformations to find mean,\n",
        "#    confidence intervals, etc., in the original friction angle space.\n",
        "#\n",
        "# Example steps:\n",
        "# shape = sigma_lnphi\n",
        "# scale = np.exp(mu_lnphi)\n",
        "# Then lognorm.mean(shape, scale=scale) can give the mean of the distribution in original units.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi8iRWq5670f"
      },
      "source": [
        "### Question 4.7\n",
        "Ten shear tests on a Tertiary clay deposit yield friction angle (φ) and cohesion (c) pairs:\n",
        "(25°, 50 kN/m²), (22°, 65 kN/m²), (32°, 18 kN/m²), (29°, 20 kN/m²), (28°, 30 kN/m²), (38°, 5 kN/m²), (36°, 6 kN/m²), (32°, 12 kN/m²), (27°, 38 kN/m²), (23°, 45 kN/m²). Investigate whether there is a correlation between friction angle and cohesion. Summarize your findings on the relationship (e.g., positive, negative, or none).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFxbrDRx670f"
      },
      "outputs": [],
      "source": [
        "# Hints and Starting Code for Question 4.7\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Approach Hints:\n",
        "# 1. Store the friction angle and cohesion data in numpy arrays or a pandas DataFrame.\n",
        "# 2. Plot φ vs. c to visualize potential correlation.\n",
        "# 3. Calculate Pearson's correlation coefficient using pearsonr or a similar function.\n",
        "# 4. Interpret whether the correlation is statistically significant and positive/negative.\n",
        "\n",
        "# Example structure:\n",
        "# phi = np.array([25, 22, 32, 29, 28, 38, 36, 32, 27, 23])\n",
        "# cohesion = np.array([50, 65, 18, 20, 30, 5, 6, 12, 38, 45])\n",
        "# correlation_coefficient, p_value = pearsonr(phi, cohesion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8KjrDs_670f"
      },
      "outputs": [],
      "source": [
        "### Question 4.8\n",
        "Monthly landslide frequency in a mountain region is documented in a histogram (Fig. 4.53). The question is whether there is a cyclic pattern or seasonal trend in landslide occurrence. Outline a method to determine if the observed frequencies are random or exhibit significant seasonality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qangEzp0670f"
      },
      "outputs": [],
      "source": [
        "# Hints and Starting Code for Question 4.8\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# Approach Hints:\n",
        "# 1. Represent the monthly frequency data in a time series format.\n",
        "# 2. Use time-series analysis, e.g., seasonal_decompose from statsmodels, to detect seasonality.\n",
        "# 3. Alternatively, consider hypothesis tests for randomness or periodicity (e.g. autocorrelation plots).\n",
        "# 4. Visualize the data to check for any repeating patterns across months or seasons.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWwwfDM1670f"
      },
      "source": [
        "### Question 4.9\n",
        "Two boreholes have been drilled on opposite sides of a fault. The objective is to reconstruct the vertical displacement across the fault. Direct markers are lacking, but variations in mean chlorite content (Fig. 4.54) may provide an indirect measure of displacement. Propose a strategy to use cross-correlation of chlorite content profiles to estimate the fault offset. Discuss any assumptions about continuity and variability of the chlorite data in the subsurface.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQxtCLct670f"
      },
      "outputs": [],
      "source": [
        "# Hints and Starting Code for Question 4.9\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import correlate\n",
        "\n",
        "# Approach Hints:\n",
        "# 1. Represent each borehole's chlorite content profile as a 1D series of numeric values at depth intervals.\n",
        "# 2. Use the scipy.signal.correlate function to compute cross-correlation between the two profiles.\n",
        "# 3. Identify the lag (depth shift) that maximizes correlation as an estimate of displacement.\n",
        "# 4. Carefully consider sampling intervals, data resolution, and boundary effects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoDB4Ovs670f"
      },
      "source": [
        "### Question 4.10\n",
        "A medieval copper mine site is being converted into residential housing, and evidence of ancient copper contamination is expected. Sea thrift (Armeria maritima) is an indicator plant for copper, so its presence was recorded along a profile in 20 m × 20 m squares. The observed shoot counts are:  \n",
        "3, 5, 11, 12, 8, 19, 22, 18, 11, 13  \n",
        "\n",
        "An experimental semivariogram is to be derived from these data as a preliminary spatial analysis. Outline how you would compute and plot a semivariogram for these shoot counts, and discuss what patterns in spatial variability might imply for copper contamination distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9wEw6TA670g"
      },
      "outputs": [],
      "source": [
        "# Hints and Starting Code for Question 4.10\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Approach Hints:\n",
        "# 1. Arrange the shoot counts in spatial order (e.g., as a function of distance along the profile).\n",
        "# 2. Calculate pairwise distances between measurement points (assuming each square is at intervals of 20 m).\n",
        "# 3. Compute the semivariogram γ(h) = 0.5 * mean[ (Z(x) - Z(x+h))^2 ] for each distance bin h.\n",
        "# 4. Plot the semivariogram (γ on the y-axis vs. distance h on the x-axis) to see if there's any spatial structure.\n",
        "# 5. Look for a sill, range, or nugget effect that might indicate how contamination (and thus sea thrift) is distributed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGJ683c_670g"
      },
      "source": [
        "# Question 4.11\n",
        "\n",
        "The following rock sequences are encountered in exploratory drilling:\n",
        "\n",
        "- Claystone (C)  \n",
        "- Conglomerate (K)  \n",
        "- Sandstone (S)  \n",
        "- Siltstone (U)\n",
        "\n",
        "(See Table 4.5 for the per-meter breakdown.)\n",
        "\n",
        "**Task:**  \n",
        "1. What is the probability that conglomerate (K) will be drilled again in the *next* meter of drilling?  \n",
        "2. What is the probability that conglomerate will be drilled again in the *next-but-one* meter of drilling?\n",
        "\n",
        "You may assume that the encountered rock types can be treated as sequential observations of a Markov process or using simpler independence assumptions (depending on your interpretation of the problem’s statement)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpfpIM83670g"
      },
      "outputs": [],
      "source": [
        "# Use this cell to start setting up your approach in Python.\n",
        "# Possible steps:\n",
        "# 1. Represent the rock sequences as a list or array.\n",
        "# 2. Construct transition probabilities from the data if needed.\n",
        "# 3. Calculate probabilities for \"next meter\" and \"next-but-one meter.\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# HINT:\n",
        "# - If you treat each meter drilled as a step in a Markov chain, you need a 4x4 transition matrix\n",
        "#   (C, K, S, U) or some simpler approach based on frequencies.\n",
        "# - Probability that K appears next might come directly from the row in the matrix corresponding\n",
        "#   to the current rock type.\n",
        "# - For the 'next-but-one' question, you might multiply transition matrices (e.g., T^2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njWawH8u670g"
      },
      "source": [
        "# Question 4.12\n",
        "\n",
        "The following orientation data (dip direction α / dip β) for bedding planes are recorded (n=14):\n",
        "\n",
        "123/25, 147/22, 120/24, 111/24, 142/26, 133/27, 135/22, 156/21,  \n",
        "110/25, 101/25, 133/20, 123/22, 145/25, 126/24\n",
        "\n",
        "**Task:**  \n",
        "1. Determine the mean resultant length.  \n",
        "2. Determine the dip direction and dip of the mean vector.  \n",
        "3. Determine the spherical standard deviation and the spherical confidence interval of the mean vector for an error probability of α = 0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtlDtkNw670g"
      },
      "outputs": [],
      "source": [
        "# Use this cell to load and process the orientation data.\n",
        "# HINT:\n",
        "# 1. Convert the dip direction/dip pairs into unit vectors in 3D space.\n",
        "# 2. Sum these vectors, and derive the resultant direction and magnitude.\n",
        "# 3. Calculate relevant statistics (standard deviation, confidence intervals) on a sphere.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Example structure:\n",
        "# data = [(123,25), (147,22), (120,24), ...]\n",
        "# Then convert each (alpha, beta) to x,y,z components on the unit sphere.\n",
        "# Summation and geometry yield the mean vector direction and length.\n",
        "#\n",
        "# You might consider using spherical to Cartesian conversions:\n",
        "# x = cos(dip) * sin(direction)\n",
        "# y = cos(dip) * cos(direction)\n",
        "# z = sin(dip)\n",
        "# (in radians)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "name": "GEOL 593_Alex M Gould_Module 3 HW_FEB232026",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}